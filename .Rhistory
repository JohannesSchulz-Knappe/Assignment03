govqual <- read.table('http://www.qogdata.pol.gu.se/data/qog_std_ts_jan16.csv')
govqual <- import('http://www.qogdata.pol.gu.se/data/qog_std_ts_jan16.csv')
library(import)
library(rio)
install.packages(rio)
install.packages('rio')
library(rio)
govqual <- import('http://www.qogdata.pol.gu.se/data/qog_std_ts_jan16.csv')
install.packages("repmis")
load <- c("dplyr", "magrittr", "readstata13", "ggplot2", "reshape",
"plyr", "repmis")
loaded <- lapply(load, require, character.only = T)
rm(list = ls()) #just to clear all
install.packages("dplyr")
install.packages("magrittr")
install.packages("readstata13")
install.packages("ggplot2")
install.packages("reshape")
install.packages("plyr")
install.packages("repmis")
load <- c("dplyr", "magrittr", "readstata13", "ggplot2", "reshape",
"plyr", "repmis")
loaded <- lapply(load, require, character.only = T)
rm(load, loaded)
data(USArrests) # to load the data
names(USArrests) # to see names of variables
dim(USArrests) # to see how big the data frame, for instance, it has 50 rows, 4 variables
arrange(USArrests, murder) # to arrage data to see at a glance which state has highest murder rate
arrange(USArrests, Murder) # to arrage data to see at a glance which state has highest murder rate
attributes(USArrests)
attributes(USArrests)$row.names
attributes(USArrests)
USArrests <- rename(USArrests, c(Assault  = "assault",
Murder   = "murder",
UrbanPop = "urbanpop",
Rape     = "rape"))
USArrests$states <- row.names(USArrests)  # creating "states"
USArrests$states <- row.names(USArrests)  # creating "states" variable
USArrests <- USArrests[, c(5,1:4)]
View(USArrests)
for (i in 1:length(names(USArrests))) {
USArrests[, i] %>%
mean() %>%
round(digits = 3) %>%
paste(names(USArrests)[i], ., '\n') %>%
cat()
}
View(i)
i
for (i in 1:length(names(USArrests))) {
USArrests[, i] %>%
median() %>%
round(digits = 3) %>%
paste(names(USArrests)[i], ., '\n') %>%
cat()
}
mean(USArrests$murder, na.rm = TRUE)
mean(USArrests$assault, na.rm = TRUE)
mean(USArrests$urbanpop, na.rm = TRUE)
mean(USArrests$rape, na.rm = TRUE)
median(USArrests$murder, na.rm = TRUE)
median(USArrests$assault, na.rm = TRUE)
median(USArrests$urbanpop, na.rm = TRUE)
median(USArrests$rape, na.rm = TRUE)
range(USArrests$murder)
range(USArrests$assault)
range(USArrests$urbanpop)
range(USArrests$rape)
summary(USArrests$murder, assault, urbanpop, rape)
summary(USArrests)
summary(USArrests)
ggplot(USArrests) +
geom_bar(aes(y = murder, x = reorder(states, -murder)),
stat = "identity") +
ylab("Murder Rate per 100,000") +
xlab("State") +
ggtitle("Murder Rate in the United States in 1973") +
theme_bw() +
theme(axis.text.x = element_text(angle = 90))
ggplot(USArrests) +
geom_bar(aes(y = rape, x = reorder(states, -rape)),
stat = "identity") +
ylab("Rape arrests (per 100,000)") +
xlab("States") +
ggtitle("Rape rate in the United States in 1973") +
theme_bw() +
coord_flip()
ggplot(USArrests) +
geom_bar(aes(y = rape, x = reorder(states, -rape)),
stat = "identity") +
ylab("Rape arrests (per 100,000)") +
xlab("States") +
ggtitle("Rape rate in the United States in 1973") +
theme_bw() +
coord_flip()
ggplot(USArrests) +
geom_histogram(aes(x = urbanpop),
colour = "black",
fill = "transparent",
binwidth = 2) +
ylab("Frequency") +
xlab("Population in % living in urban areas") +
theme_bw()
states <- row.names(USArrests)
barplot(USArrests$murder,
names.arg = states,
las = 2,
ylab = "Murder rate per 100,000 in different states",
main = "Murder rate in the United States in 1973")
barplot(USArrests$murder,
names.arg = reorder(states, -murder),
las = 2,
ylab = "Murder rate per 100,000 in different states",
main = "Murder rate in the United States in 1973")
barplot(USArrests$murder,
names.arg = reorder(states, -Murder),
las = 2,
ylab = "Murder rate per 100,000 in different states",
main = "Murder rate in the United States in 1973")
barplot(USArrests$murder,
names.arg = states,
las = 2,
ylab = "Murder rate per 100,000 in different states",
main = "Murder rate in the United States in 1973")
hist(USArrests$assault,
main = 'Violent Assault by USA States (per 100,000) in 1973',
xlab = 'numeric	 Assault arrests (per 100,000')
hist(USArrests$assault,
main = 'Violent Assault by USA States (per 100,000) in 1973',
xlab = 'numeric	 Assault arrests (per 100,000)',
ylab = 'Number of States'
hist(USArrests$assault,
main = 'Violent Assault by USA States (per 100,000) in 1973',
xlab = 'numeric	 Assault arrests (per 100,000)',
ylab = 'Number of States')
hist(USArrests$assault,
main = 'Violent Assault by USA States (per 100,000) in 1973',
xlab = 'numeric	 Assault arrests (per 100,000)')
library(dplyr)
library(rvest)
install.packages('rvest')
library(rvest)
URL <- http://www.bbc.com/sport/winter-olympics/2014/medals/countries
URL <- 'http://www.bbc.com/sport/winter-olympics/2014/medals/countries'
# Get and parse expenses_table from the webpage
MedalTable <- URL %>% read_html() %>%
html_nodes('#medals-table') %>%
html_table() %>%
as.data.frame
View(MedalTable)
# Get and parse expenses_table from the webpage
MedalsTable <- URL %>% read_html() %>%
html_nodes('medals-table') %>%
html_table() %>%
as.data.frame
View(MedalsTable)
MedalsTable <- URL %>% read_html() %>%
html_nodes('medals-table__table') %>%
html_table() %>%
as.data.frame
View(MedalsTable)
MedalsTable <- URL %>% read_html() %>%
html_nodes('#medals-table__table') %>%
html_table() %>%
as.data.frame
View(MedalsTable)
rm(list = ls())
library(dplyr)
library(rvest)
URL <- 'http://www.bbc.com/sport/winter-olympics/2014/medals/countries'
MedalsTable <- URL %>% read_html() %>%
html_nodes('#orb-nav-links') %>%
html_table() %>%
as.data.frame
MedalsTable <- URL %>% read_html() %>%
html_nodes('#medals_table') %>%
html_table() %>%
as.data.frame
html_nodes('#medals__table') %>%
MedalsTable <- URL %>% read_html() %>%
html_nodes('#medals__table') %>%
html_table() %>%
as.data.frame
MedalsTable <- URL %>% read_html() %>%
html_nodes('#medals_table__table') %>%
html_table() %>%
as.data.frame
MedalsTable <- URL %>% read_html() %>%
html_nodes('#orb-nav-links') %>%
html_table() %>%
as.data.frame
MedalsTable <- URL %>% read_html() %>%
html_nodes('#medals-table__total--silver') %>%
html_table() %>%
as.data.frame
MedalsTable <- URL %>% read_html() %>%
html_nodes('#table') %>%
html_table() %>%
as.data.frame
# Get and parse expenses_table from the webpage
MedalsTable <- URL %>% read_html() %>%
html_nodes('table') %>%
html_table() %>%
as.data.frame
View(MedalsTable)
head(MedalsTable)
drop(NA.$MedalsTable)
drop(MedalsTable$NA.)
head(MedalsTable)
MedalsTable$NA. <- NULL
head(MedalsTable)
MedalsTable <- URL %>% read_html() %>%
html_nodes('table') %>%
html_table() %>%
as.data.frame
head(MedalsTable)
View(MedalsTable)
MedalsTable$NA. <- NULL
View(MedalsTable)
TotalMedals <- arrange(MedalsTable, desc(Total))
head(TotalMedals)
tables <- URL %>% read_html() %>%
html_nodes('table') %>%
html_table()
possible_dir <- c('C:/Users/Johannes SK/Dropbox/Studium/Spring2016/CollaborativeResearch/Assignment02',
'C:/Users/User/Documents/GitHub/Assignment02')
toBibtex(citation())
# Saxony Anhalt 2016
sa16 <- read.csv('http://www.statistik.sachsen-anhalt.de/wahlen/lt16/erg/csv/lt16dat2.csv',
sep = ';')
View(sa16)
rp16 <- read.csv('Data_Files/wahlnachtanalyse-lw2016_page_65.csv', header = FALSE, sep = ';')
trees <- c('Jomon Sugi', 'Huon Pine')
View(trees)
str_split_fixed(trees, pattern = ' ', n = 2)
library(stringr)
str_split_fixed(trees, pattern = ' ', n = 2)
View(trees)
str_split_fixed(trees, pattern = ' ', n = 2)
trees2 <- data.frame[str_split_fixed(trees, pattern = ' ', n = 2)]
trees2 <- str_split_fixed(trees, pattern = ' ', n = 2)
View(trees2)
trees <- c('1 Sugi', '2 Pine')
View(trees)
trees <- c('1 Sugi', '2 Pine')
View(trees)
trees2 <- str_split_fixed(trees, pattern = ' ', n = 2)
View(trees2)
rp16 <- source_XlsxData('http://www.wahlen.rlp.de/ltw/wahlen/2016_Landtagswahlergebnisse_Wahlkreise_Endgueltig.xlsx',
2, cache = TRUE)
library(xlsx)
rp16 <- source_XlsxData('http://www.wahlen.rlp.de/ltw/wahlen/2016_Landtagswahlergebnisse_Wahlkreise_Endgueltig.xlsx',
2, cache = TRUE)
rp16 <- source_XlsxData('http://www.wahlen.rlp.de/ltw/wahlen/2016_Landtagswahlergebnisse_Wahlkreise_Endgueltig.xlsx',
2, cache = TRUE)
library(xlsx)
rp16 <- source_XlsxData('http://www.wahlen.rlp.de/ltw/wahlen/2016_Landtagswahlergebnisse_Wahlkreise_Endgueltig.xlsx',
2, cache = TRUE)
library(repmis)
library(rio)
rp16 <- source_XlsxData('http://www.wahlen.rlp.de/ltw/wahlen/2016_Landtagswahlergebnisse_Wahlkreise_Endgueltig.xlsx',
2, cache = TRUE)
View(rp16)
rp16 <- source_XlsxData('http://www.wahlen.rlp.de/ltw/wahlen/2016_Landtagswahlergebnisse_Wahlkreise_Endgueltig.xlsx',
2, cache = TRUE)
rp16 <- import('http://www.wahlen.rlp.de/ltw/wahlen/2016_Landtagswahlergebnisse_Wahlkreise_Endgueltig.xlsx',
2, cache = TRUE)
rp16 <- import('http://www.wahlen.rlp.de/ltw/wahlen/2016_Landtagswahlergebnisse_Wahlkreise_Endgueltig.xlsx', format = xlsx,
2, cache = TRUE)
2, cache = TRUE)
rp16 <- import('http://www.wahlen.rlp.de/ltw/wahlen/2016_Landtagswahlergebnisse_Wahlkreise_Endgueltig.xlsx', format = 'xlsx',
2, cache = TRUE)
rp16 <- import('http://www.wahlen.rlp.de/ltw/wahlen/2016_Landtagswahlergebnisse_Wahlkreise_Endgueltig.xlsx', format = 'xlsx',
2)
View(rp16)
rp16 <- import('http://www.wahlen.rlp.de/ltw/wahlen/2016_Landtagswahlergebnisse_Wahlkreise_Endgueltig.xlsx', format = 'xlsx',
sheetIndex = 2)
rm(list = ls())
## 1.2 Load packages for dataset creation
# Create vector of used packages
packages <- c('repmis', 'rvest', 'plyr', 'rio', 'xlsx')
# Install packages that are not already installed
for (p in packages) {
if (p %in% installed.packages()[,1]) require(p, character.only=TRUE)
else {
install.packages(p)
require(p, character.only=TRUE)
}
}
rm(p)
# Load packages
loaded <- lapply(packages, require, character.only = TRUE)
rm(loaded)
## 1.3 Set the working directory
# Create list of commonly used working directories (update, if needed)
possible_dir <- c('C:/Users/Johannes SK/Dropbox/Studium/Spring2016/CollaborativeResearch/Assignment03',
'C:/Users/User/Documents/GitHub/Assignment03')
# Set to first valid directory in the possible_dir vector
set_valid_wd(possible_dir)
# remove possible_dir vector
rm(possible_dir)
## 1.4 Create bibTeX file for the used packages
LoadandCite(packages, file = 'Packages1.bib')
rm(packages)
#-----------------------------------------#
# 2. Run gathering and cleaning R files   #
#-----------------------------------------#
# Dynamically run R files in this order
## 2.1 Electoral data gathering
source("data_gathering/election_data_gathering.R")
## 2.2 Structural data gathering
source("data_gathering/structural_data_gathering.R")
## 2.3 Electoral data cleaning
source("data_cleaning/election_data_cleaning.R")
## 2.4 Structural data cleaning
source("data_cleaning/structural_data_cleaning.R")
#-----------------------------------------#
# 3. Merge the data                       #
#-----------------------------------------#
# Merge the data into the final data frame
Data <- merge(data.election, edu, "ID")
Data <- merge(Data, gdp, "ID")
Data <- merge(Data, unemp, "ID")
Data <- merge(Data, refugee, "ID")
# Remove used data frames
rm(data.election)
rm(edu)
rm(gdp)
rm(unemp)
rm(refugee)
# Save Data as file in repository
save(Data, file = "Data.Rda")
View(Data)
rm(list = ls())
## 1.2 Load packages for data analysis
# Create vector of used packages
packages <- c("repmis", "plyr", "xtable", "texreg", "stargazer",
"magrittr", "readstata13", "ggplot2", "reshape")
# Install packages that are not already installed
for (p in packages) {
if (p %in% installed.packages()[,1]) require(p, character.only=TRUE)
else {
install.packages(p)
require(p, character.only=TRUE)
}
}
rm(p)
# Load packages
loaded <- lapply(packages, require, character.only = TRUE)
rm(loaded)
## 1.3 Set the working directory
# Create list of commonly used working directories (update, if needed)
possible_dir <- c('C:/Users/Johannes SK/Dropbox/Studium/Spring2016/CollaborativeResearch/Assignment03',
'C:/Users/User/Documents/GitHub/Assignment03')
# Set to first valid directory in the possible_dir vector
set_valid_wd(possible_dir)
# remove possible_dir vector
rm(possible_dir)
## 1.4 Create bibTeX file for the used packages
LoadandCite(packages, file = 'Packages2.bib')
rm(packages)
## 1.5 Create and load the dataset
load('Data.Rda')
#--------------------------------------#
# 2. Descriptive Statistics            #
#--------------------------------------#
## 2.1 Mean values
# Create mean values for variables
mean(Data$vote.AfD, na.rm = TRUE)
mean(Data$GDP.capita, na.rm = TRUE)
mean(Data$unempl.rate, na.rm = TRUE)
mean(Data$nodegree.ratio, na.rm = TRUE)
mean(Data$n.refugees, na.rm = TRUE)
mean(Data$abitur.ratio, na.rm = TRUE)
# Loop function for finding "mean" for all the variables
for (i in 4:length(names(Data))) {
Data[, i] %>%
mean() %>%
round(digits = 2) %>%
paste(names(Data)[i], ., '\n') %>% # the . directs the
cat()
}
## 2.2 Summary statistics of all variables
# Assign variable labels
summary_labels <- c('Vote share of AfD in 2016', 'Vote turnout in 2011',
'Vote share of CDU in 2011', 'Vote share of Greens in 2011',
'Vote share of SPD in 2011', 'Vote share of FDP in 2011',
'Vote share of Linke in 2011', 'High school ratio',
'No degree ratio', 'GDP per capita',
'Unemployment rate', 'Number of refugees')
# Create summary statistics table
stargazer(Data[4:16],
title = 'Summary statistics of all variables',
covariate.labels = summary_labels,
font.size = 'small',
digit = 2,
out = 'outputs/tables/summary_statistics.tex')
## 2.3 Histograph using ggplot to show frequency distribution of the number of refugees
ggplot(Data) +
geom_histogram(aes(x = n.refugees),
colour = "black",
fill = "transparent",
binwidth = 30) +
ylab("Frequency") +
xlab("Number of refugees per district") +
theme_light()
## 2.4 AfD vote share by district
ggplot(Data) +
geom_bar(aes(y = vote.AfD, x = reorder(district.name, -vote.AfD)),
stat = "identity") +
ylab("AfD vote share") +
xlab("Districts of BW, RP, and SA") +
ggtitle("Vote share of AfD in districts of BW, RP, and SA") +
theme_bw() +
theme(axis.text.x = element_text(angle = 90))
out = 'outputs/figures/AfD_voteshare_in_RP_BW_SA.pdf'
## 2.5 Correlation matrix in table format
correlation.matrix <- cor(Data[, c("vote.AfD","lag.CDU",
"lag.SPD","GDP.capita",
"unempl.rate","n.refugees",
"abitur.ratio")])
stargazer(correlation.matrix,
title = "Correlation Matrix",
font.size = 'small',
out = 'outputs/tables/correlation_matrix.tex')
#--------------------------------------#
# 3. Initial correlation analysis      #
#--------------------------------------#
## 3.1 Correlation of AfD's vote share and unemployment rate
# The result shows statistically significant positve
# correlation of increase in unemployment rate with
# the increase in AfD vote share in the three states
ggplot2::ggplot(Data, aes(log(x = unempl.rate), vote.AfD)) +
xlab("Unemployment rate in RP, BW, and SA") +
ylab("AfD vote share (in %)") +
geom_point() + geom_smooth() +
theme_bw()
# Test for statistical significance of above correlation
cor.test(log(Data$vote.AfD), Data$unempl.rate)
## 3.2 Correlation of AfD vote share and GDP per capita
# The result shows statistically significant negative
# coorelation of increase in Per capita GDP
# with AfD vote share
ggplot2::ggplot(Data,aes(log(x = GDP.capita), vote.AfD)) +
xlab("GDP per capita in RP, BW, and SA") +
ylab("AfD vote share (in %)") +
geom_point() + geom_smooth() +
theme_bw()
# Test for statistical significance of above correlation
cor.test(log(Data$vote.AfD), Data$GDP.capita)
## 3.3 Correlation of AfD vote share and No. of Refugees
# The result shows positive coorelation of
# increase no. of refugees with with AfD's vote share,
# though it is not statistically significant (most likely
# due to outdated refugee numbers)
ggplot2::ggplot(Data,aes(log(x = n.refugees), vote.AfD)) +
xlab("Number of refugees in RP, BW, and SA") +
ylab("AfD vote share (in %)") +
geom_point() + geom_smooth() +
theme_light()
# Test for statistical significance of above correlation
cor.test(log(Data$vote.AfD), Data$n.refugees)
#--------------------------------------#
# 4. Inferential Analysis              #
#--------------------------------------#
# Create series of regression models
reg1 <- lm(vote.AfD ~ GDP.capita + unempl.rate + n.refugees, data = Data)
reg2 <- lm(vote.AfD ~ GDP.capita + unempl.rate + n.refugees + lag.CDU +
lag.SPD + lag.turnout, data = Data)
reg3 <- lm(vote.AfD ~ GDP.capita + unempl.rate + n.refugees +
abitur.ratio + nodegree.ratio, data = Data)
reg4 <- lm(vote.AfD ~ GDP.capita + unempl.rate + n.refugees
+ lag.CDU + lag.SPD + lag.turnout + abitur.ratio +
nodegree.ratio, data = Data)
# Assign variable labels
var_labels <- c('GDP per capita', 'Unemployment rate', 'No. of Refugees',
'Vote share of CDU in 2011 election',
'Vote share of SPD in 2011 election',
'High School Ratio', 'University Degree ratio', '(Intercept)')
# Create regression table
stargazer::stargazer(reg1, reg2, reg3, reg4,
omit = 'as.factor*',
omit.stat = c('f', 'ser'), # to nicely fits on the page
out.header = F,
title = 'Determinants of the vote share of the Alternative for Germany',
dep.var.labels = 'Vote share of AfD',
covariate.labels = var_labels,
label = 'AfD_voteshare',
add.lines = list(c('District FE?', rep('NO', 4))),
font.size = 'small',
digit = 2,
out = 'outputs/tables/afd_voteshare_regressions.tex')
